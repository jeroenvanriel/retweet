{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Classification\n",
    "This is the second chapter of this series. In this notebook we aim to train a classifier based on the word embeddings we pretrained in the previous notebook. Then we will store the classifier to conduct emotional analysis on real-time tweets, which will be covered in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import helpers.pickle_helpers as ph\n",
    "import time\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "First, let's declare our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50 \n",
    "DELTA = 0.5\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Let's import and prepare the data as was done in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ph.load_from_pickle(directory=\"data/datasets/df_grained_tweet_tr.pkl\")\n",
    "test_data = ph.load_from_pickle(directory=\"data/datasets/df_grained_tweet_te_unbal.pkl\")\n",
    "\n",
    "train_data.rename(index=str, columns={\"emo\":\"emotions\", \"sentence\": \"text\"}, inplace=True);\n",
    "test_data.rename(index=str, columns={\"emo\":\"emotions\", \"sentence\": \"text\"}, inplace=True);\n",
    "\n",
    "train_data.text = train_data.text.str.replace(\" <hashtag>\", \"\")\n",
    "test_data.text = test_data.text.str.replace(\" <hashtag>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^\\'\\\"A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = [y for y in string if len(y) > 3 and y.find('nbsp') < 0]\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.text = train_data.text.apply(lambda d: clearstring(d))\n",
    "test_data.text = test_data.text.apply(lambda d: clearstring(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Word Embeddings\n",
    "Here is the code for importing the word embeddings we pretrained in the previous chapter. Notice that we are also importing the vocabulary. See below how handy the vocabulary is to inspect our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load word embeddings and accompanying vocabulary\n",
    "wv = ph.load_from_pickle(\"data/hashtags_word_embeddings/es_py_cbow_embeddings.p\")\n",
    "vocab = ph.load_from_pickle(\"data/hashtags_word_embeddings/es_py_cbow_dictionary.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5389905 , -0.8261296 , -1.8023891 , -0.8072674 , -0.6313184 ,\n",
       "       -1.3096205 ,  1.6170695 ,  1.8171018 ,  0.05804818,  1.5923933 ,\n",
       "        1.2208248 , -0.08000907,  1.4284078 ,  0.5594934 ,  0.8742701 ,\n",
       "        0.04409672, -0.51616585, -0.26882973,  0.2614767 ,  1.7617252 ,\n",
       "       -0.7654648 , -0.1121751 ,  0.6021578 , -2.7278464 , -1.5101068 ,\n",
       "        1.9514263 ,  0.9859432 , -2.0553567 ,  0.52864003, -1.5633332 ,\n",
       "       -2.329722  ,  0.33874342,  0.9558916 ,  0.9637566 ,  0.72352   ,\n",
       "       -0.60107934,  1.2980587 ,  1.3291203 ,  0.08595378, -0.96753865,\n",
       "       -0.47979838, -1.4262284 ,  0.80548376,  0.94358546, -0.85197926,\n",
       "       -1.5562207 , -0.28793994, -0.21579984, -0.6607775 , -0.21598966,\n",
       "        1.6049399 , -0.343651  , -0.0540315 , -2.1718023 , -0.98242474,\n",
       "       -1.6945462 , -1.3239328 ,  1.6394376 , -1.1029811 ,  0.42646387,\n",
       "       -1.0574629 , -0.4617092 , -1.0275363 ,  1.7248987 , -0.05921336,\n",
       "        0.9992472 ,  0.7281742 ,  1.0187635 ,  1.8406339 , -2.0048149 ,\n",
       "        2.6621861 ,  0.80933565,  0.65741915, -0.1611871 ,  0.72472906,\n",
       "        1.483416  , -0.800681  , -0.6170338 ,  0.9091752 ,  0.35176483,\n",
       "       -1.4197102 ,  0.73179495,  1.2767175 , -0.74212426, -0.52197933,\n",
       "       -1.8342316 , -0.8961808 ,  1.1606023 , -2.0411768 , -1.3687735 ,\n",
       "       -2.1972206 ,  0.16410299, -0.6888266 , -1.58254   ,  0.4490404 ,\n",
       "        2.5568488 ,  0.9290964 , -0.9500061 , -0.25545642, -0.19501002,\n",
       "       -0.9169069 ,  1.7392551 ,  0.8232341 ,  0.93090016, -1.2818229 ,\n",
       "       -0.18206023,  0.5242739 ,  0.6704099 ,  1.7621306 , -1.0661116 ,\n",
       "        0.850737  ,  0.02583216, -1.7723794 ,  1.7245288 ,  2.6550083 ,\n",
       "       -0.89468575, -1.0228765 ,  1.4283719 ,  1.4670846 , -1.108844  ,\n",
       "        1.4647324 ,  1.5425268 ,  0.56075734,  2.5217469 , -0.37765834,\n",
       "        0.3549512 ,  1.5589453 ,  0.5800641 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### eg. to obtain embedding for token\n",
    "wv[vocab[\"feel\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Label Binarization\n",
    "A very important step before training our classifier, is to make sure the data is in the right format so it becomes easy for us to feed the data into the model. In the code below we will tokenize our dataset, in particular the inputs. Then we will also perform binarization on the target values so as to obtain one-hot vectors that will uniquely represent the target of each sentence or tweet. We also do some additional pre-processing which you can follow below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unknown_words(tokens):\n",
    "    return [t for t in tokens if t in vocab]\n",
    "\n",
    "def check_size(c, size):\n",
    "    if len(c) <= size:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "### tokens and tokensize\n",
    "train_data[\"tokens\"] = train_data.text.apply(lambda t: remove_unknown_words(t.split()))\n",
    "train_data[\"tokensize\"] = train_data.tokens.apply(lambda t: len(t))\n",
    "test_data[\"tokens\"] = test_data.text.apply(lambda t: remove_unknown_words(t.split()))\n",
    "test_data[\"tokensize\"] = test_data.tokens.apply(lambda t: len(t))\n",
    "\n",
    "### filter by tokensize\n",
    "train_data = train_data.loc[train_data[\"tokens\"].apply(lambda d: check_size(d, 7)) != False].copy()\n",
    "test_data = test_data.loc[test_data[\"tokens\"].apply(lambda d: check_size(d, 7)) != False].copy()\n",
    "\n",
    "### sorting by tokensize\n",
    "train_data.sort_values(by=\"tokensize\", ascending=True, inplace=True)\n",
    "test_data.sort_values(by=\"tokensize\", ascending=True, inplace=True)\n",
    "\n",
    "### resetting index\n",
    "train_data.reset_index(drop=True, inplace=True);\n",
    "test_data.reset_index(drop=True, inplace=True);\n",
    "\n",
    "### Binarization\n",
    "emotions = list(set(train_data.emotions.unique()))\n",
    "num_emotions = len(emotions)\n",
    "\n",
    "### binarizer\n",
    "mlb = preprocessing.MultiLabelBinarizer()\n",
    "\n",
    "train_data_labels =  [set(emos) & set(emotions) for emos in train_data[['emotions']].values]\n",
    "test_data_labels =  [set(emos) & set(emotions) for emos in test_data[['emotions']].values]\n",
    "\n",
    "y_bin_emotions = mlb.fit_transform(train_data_labels)\n",
    "test_y_bin_emotions = mlb.fit_transform(test_data_labels)\n",
    "\n",
    "train_data['bin_emotions'] = y_bin_emotions.tolist()\n",
    "test_data['bin_emotions'] = test_y_bin_emotions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sample input\n",
    "Once we have processed our data, let's look at an example of how we will converting sentences into input vectors, which are basically word vectors concatenated to represent the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = [wv[vocab[w]] for w in \"this feels fantastic\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.7762969e-01,  4.1182911e-01,  1.5915717e+00,  1.9623135e-01,\n",
       "         1.4823467e-01,  3.4592927e-02,  1.0979089e-01, -5.4003459e-01,\n",
       "         5.5145639e-01, -2.0645244e-01,  6.2708288e-01,  1.9114013e+00,\n",
       "         4.1743749e-01,  4.8000565e-01,  1.3688921e+00, -6.0899270e-01,\n",
       "        -8.2222080e-01, -1.6738379e-01,  2.5278423e-03, -4.4002768e-01,\n",
       "        -1.7636645e-01,  3.1228867e-01,  8.5302269e-01, -5.5778861e-02,\n",
       "        -9.6316218e-01,  6.3835210e-01,  1.1264894e+00, -7.7165258e-01,\n",
       "         1.7387373e+00,  1.3290544e+00, -2.6808953e-01,  2.6583406e-01,\n",
       "         1.7067311e+00,  4.0209743e-01,  1.9354068e+00, -4.4382878e-02,\n",
       "        -1.7041634e+00, -2.1780021e+00,  6.2105244e-01,  4.5051843e-01,\n",
       "        -9.4019301e-02, -1.6840085e-01, -6.8932152e-01, -8.8215894e-01,\n",
       "        -1.4211287e+00, -6.9710428e-01,  9.1269486e-02, -1.3960580e+00,\n",
       "        -2.6473520e+00,  1.2631515e-01,  1.0753033e+00, -1.7343637e+00,\n",
       "        -1.2398950e+00, -1.8989055e-01,  5.5069500e-01, -9.9274379e-01,\n",
       "        -7.4581426e-01,  1.9070454e+00, -2.7693167e-02, -9.6485667e-02,\n",
       "         3.6455104e+00, -2.2448828e+00, -2.3194687e+00, -5.6355500e-01,\n",
       "        -2.2364409e+00, -1.3884341e+00,  6.8607783e-01,  6.3522869e-01,\n",
       "         1.6772349e+00,  9.3361482e-02,  1.5434825e+00, -8.9733368e-01,\n",
       "        -2.0110564e-01,  5.5500650e-01, -2.7845064e-01,  8.5825706e-01,\n",
       "         2.6179519e-01, -1.4814560e-01, -5.7858503e-01,  5.2921659e-01,\n",
       "        -6.2351793e-01,  1.1778877e+00,  6.9542038e-01,  1.8816992e+00,\n",
       "         3.1759745e-01, -4.7993168e-01,  6.5814179e-01,  8.7885934e-01,\n",
       "         6.0468066e-02,  7.4128270e-02,  2.2988920e+00,  2.1285081e+00,\n",
       "         7.0240453e-02, -7.6330572e-01,  8.3526218e-01,  6.0745466e-01,\n",
       "        -1.0194540e+00, -2.1956379e+00, -1.2714338e+00,  6.3572550e-01,\n",
       "         6.6295260e-01, -8.3488572e-01, -3.4988093e-01, -3.5540792e-01,\n",
       "         5.4124153e-01, -7.7268988e-01,  1.4683855e-01, -7.3003507e-01,\n",
       "        -8.1091434e-01, -1.0907569e+00,  7.5887805e-01, -1.1122453e+00,\n",
       "        -1.6199481e+00, -1.3784732e+00, -6.3396573e-02,  3.2632509e-01,\n",
       "         1.0684365e-01,  6.0308921e-01,  4.6167067e-01, -2.0168118e+00,\n",
       "        -6.7048740e-01,  1.6356069e-01, -5.4351605e-02, -5.2482843e-01,\n",
       "         2.2043006e+00, -6.8451458e-01,  5.9733611e-01,  6.6534078e-01],\n",
       "       dtype=float32),\n",
       " array([-3.4321475e-01, -9.7226053e-02,  2.8605098e-01,  1.6047080e+00,\n",
       "        -3.6453772e-01, -1.3920774e+00, -2.1250713e+00,  1.3125460e-01,\n",
       "        -1.1453985e+00,  5.4572320e-01,  5.2630770e-01, -1.8370697e-01,\n",
       "         1.4765236e+00,  1.1878918e+00, -4.0682489e-01, -4.7836415e-02,\n",
       "        -5.4636401e-01,  1.9224505e+00, -2.6708531e-01,  5.0754392e-01,\n",
       "        -6.4190727e-01,  6.8657053e-01, -6.4917213e-01,  5.0319391e-01,\n",
       "        -4.8732966e-01,  1.8009869e+00, -2.3410184e+00, -9.4244355e-01,\n",
       "        -9.2588544e-01,  7.6922643e-01, -7.4314862e-01,  7.0185089e-01,\n",
       "        -1.0966054e+00,  7.8724593e-01,  7.5844818e-01,  9.1872163e-02,\n",
       "         7.0011061e-01,  4.7920561e-01, -1.5113609e-02,  1.4994408e+00,\n",
       "        -1.0265969e+00,  2.9074928e-01, -7.6647228e-01,  1.8470247e+00,\n",
       "        -8.4488952e-01, -1.3199706e+00, -4.4321135e-01,  5.3276116e-01,\n",
       "         1.8265551e-01,  1.1034945e+00,  3.8836792e-01,  2.6915863e-01,\n",
       "        -6.7549026e-01, -1.2705921e-01, -4.9914065e-01, -3.7022445e+00,\n",
       "         1.1977068e+00,  3.4566635e-01,  5.5629976e-02,  1.3779374e+00,\n",
       "        -3.9924735e-01, -1.2794230e+00,  3.4014046e+00, -1.2588968e+00,\n",
       "        -1.6168836e-01,  8.2324558e-01,  2.9140648e-01,  2.2544200e+00,\n",
       "         1.4198905e+00, -2.7008796e+00,  1.5832986e+00,  1.3438987e-03,\n",
       "         4.7332349e-01,  1.9437153e+00,  8.7838221e-01,  1.3765662e+00,\n",
       "         6.4651889e-01, -1.0945044e-01,  4.6745947e-01, -7.7465302e-01,\n",
       "        -1.7219128e-01,  1.3659716e-01,  1.4069235e+00, -1.2043966e+00,\n",
       "         2.7390096e-01, -9.2881405e-01,  7.3064059e-02, -6.9506335e-01,\n",
       "        -2.2899912e-01, -3.2435477e+00, -2.0895963e+00,  1.0968444e+00,\n",
       "         7.4347031e-01, -3.1055303e+00, -7.6739632e-02,  4.2136496e-01,\n",
       "         3.3838820e-01, -4.1653013e-01,  1.0817224e-01, -2.2449881e-02,\n",
       "         7.2924626e-01,  4.5947462e-01,  5.7326639e-01, -1.9229509e-01,\n",
       "        -1.7776063e-01,  4.1691759e-01, -3.6446020e-01, -1.5269613e-02,\n",
       "        -1.6729140e+00,  6.9680309e-01,  1.0556157e+00,  1.0876462e+00,\n",
       "         9.8904811e-02,  1.4382801e+00,  1.7168192e+00,  1.8068274e+00,\n",
       "         2.4255323e-01, -1.0590203e+00,  1.0824920e+00, -2.5140762e+00,\n",
       "        -2.3148799e-01,  4.1911473e+00, -3.4231823e-02,  1.5553576e+00,\n",
       "         2.7134141e-01,  2.6498488e-01, -2.5449184e-01, -1.8989407e+00],\n",
       "       dtype=float32),\n",
       " array([ 0.8383601 , -0.02155342, -0.21082091,  0.6485529 , -0.59349656,\n",
       "        -0.166402  ,  1.3000834 , -0.07898946, -0.16624215,  0.52123684,\n",
       "         0.05233976, -1.5532598 ,  0.01666474,  0.797122  , -0.7451202 ,\n",
       "        -1.6641759 ,  1.2602216 , -2.0044358 ,  0.68592983,  0.7536933 ,\n",
       "         1.0812731 ,  2.10356   , -1.7539785 , -0.6635254 , -0.7465607 ,\n",
       "         0.2638522 , -0.3235982 ,  1.4076495 , -0.09119514,  2.086436  ,\n",
       "        -0.4526414 ,  0.26831234,  0.24467596,  2.33805   , -0.7017719 ,\n",
       "        -0.6682133 , -0.9301834 ,  0.21346547, -1.0819333 ,  0.03980344,\n",
       "        -0.07848723,  0.716963  , -0.28034478,  0.6563167 , -0.99363357,\n",
       "         0.71183956,  0.05822359, -1.6912135 , -2.4925132 , -0.5482579 ,\n",
       "        -0.67647994,  1.3980678 ,  2.86393   ,  1.2885548 , -1.5518631 ,\n",
       "        -1.1034924 , -1.1662406 ,  0.3353053 ,  0.19297248, -0.95059246,\n",
       "         0.31902936, -1.0137295 , -1.279213  , -0.5329634 , -0.07975607,\n",
       "        -0.19864069,  0.6106306 ,  1.0557752 , -0.878621  , -0.8509309 ,\n",
       "        -0.12062441, -0.27696317,  2.2124147 ,  1.9911683 ,  0.7381984 ,\n",
       "        -0.469987  , -1.6558627 , -0.0847896 , -1.5840882 ,  0.74699026,\n",
       "        -0.13173659,  0.96634436, -1.3921932 , -0.16244002,  0.7752265 ,\n",
       "        -0.23255356, -0.44541982, -2.2467227 ,  0.10506741, -0.20535523,\n",
       "        -0.09891574, -0.35552487,  0.13457903, -0.18867804, -0.04975915,\n",
       "         0.5091362 , -2.1489737 ,  0.84570265,  1.2204372 , -1.2863662 ,\n",
       "        -1.1997837 , -0.1355166 , -1.842612  ,  0.27185363, -0.43057394,\n",
       "         0.9251916 , -0.45085236,  0.65534955, -1.4492592 , -0.7060368 ,\n",
       "         0.58963746, -1.9130523 ,  0.74782646,  0.99171853, -0.42570722,\n",
       "        -0.73163205,  2.2265303 ,  1.0439353 ,  0.21321568,  0.70397234,\n",
       "        -0.41201043, -1.3467301 , -0.3377973 ,  1.7296644 , -2.1833317 ,\n",
       "        -1.9238352 ,  0.00673127,  1.1287643 ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching by Bucketing approach\n",
    "Here is the code to generate batches for our training. This code is a little bit different from the batching approach we used to train our embeddings. Here we are going to generate batches of input sentences. In addition, we will also use a bucketing approach, which is basically a trick to generate more efficient batches that are of similar size. You don't need to know more about the batching for now, just that it is needed for training. We will explain the purpose of bucketing more in details in a future chapter of this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### renders embeddings with paddings; zeros where missing tokens\n",
    "def generate_embeds_with_pads(tokens, max_size):\n",
    "   \n",
    "    padded_embedding = []\n",
    "    for i in range(max_size):\n",
    "        if i+1 > len(tokens): # do padding\n",
    "            padded_embedding.append(list(np.zeros(EMBEDDING_DIM)))\n",
    "        else: # do embedding for existing tokens\n",
    "            padded_embedding.append(list(wv[vocab[tokens[i]]]))  \n",
    "    return padded_embedding\n",
    "\n",
    "### generate the actual batches\n",
    "def generate_batches(data, batch_size):\n",
    "    actual_batches = math.ceil(len(data) / batch_size)\n",
    "    bins = np.linspace(0, len(data), actual_batches + 1) # this renders actual batches bins of size batch_size\n",
    "    groups = data.groupby(np.digitize(data.index, bins))\n",
    "    \n",
    "    groups_indices = groups.indices\n",
    "    groups_maxes = groups.max().tokensize\n",
    "    \n",
    "    return groups.indices, groups_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's set up our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoNet(torch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, embedding_dim, output_size, dropout):\n",
    "        super(EmoNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.keep_prob = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = num_layers\n",
    "        self.output = output_size\n",
    "        \n",
    "        self.dropout  = nn.Dropout(p=self.keep_prob)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=self.embedding_dim,\n",
    "                                 hidden_size=self.hidden_size, \n",
    "                                 num_layers=self.nlayers,\n",
    "                                 dropout=self.keep_prob)\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # batch_size X seq_len X embedding_dim -> seq_len, batch_size, embedding_dim\n",
    "        X = inputs.permute(1,0,2)\n",
    "        self.rnn.flatten_parameters()\n",
    "        output, hidden = self.rnn(X)\n",
    "        (_, last_state) = hidden      \n",
    "        out = self.dropout(output[-1])  \n",
    "        out = self.linear(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretesting with one batch sample\n",
    "Let's test the model to make sure that we are getting the right output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0232, -2.0691, -2.1157, -2.0892, -2.1025, -2.0685, -2.0496, -2.1217],\n",
      "        [-2.0673, -2.1060, -2.0852, -2.0269, -2.1725, -2.0424, -2.0406, -2.1025],\n",
      "        [-2.0391, -2.1295, -2.1221, -2.0777, -2.0516, -2.0863, -2.0247, -2.1099],\n",
      "        [-1.9732, -2.0941, -2.1539, -2.1623, -2.0857, -2.0165, -2.0270, -2.1399],\n",
      "        [-2.0045, -2.1246, -2.2071, -2.1499, -2.1643, -1.9416, -1.9807, -2.0957],\n",
      "        [-2.0669, -2.0434, -2.0972, -2.1399, -2.1139, -2.0600, -1.9759, -2.1500],\n",
      "        [-2.0779, -2.0254, -2.1648, -2.0490, -2.1991, -1.9826, -2.0161, -2.1418],\n",
      "        [-2.0564, -2.1175, -2.0688, -2.1243, -2.1005, -2.0728, -1.9879, -2.1144],\n",
      "        [-2.0580, -2.0817, -2.1553, -2.0956, -2.0416, -2.0482, -2.0393, -2.1219],\n",
      "        [-2.0021, -2.0735, -2.1122, -2.0266, -2.1404, -2.0821, -2.0176, -2.1964],\n",
      "        [-2.0476, -2.1002, -2.1645, -2.1462, -2.0558, -2.0172, -1.9741, -2.1467],\n",
      "        [-2.0600, -2.0664, -2.0646, -2.1640, -2.1037, -2.0233, -2.0344, -2.1271],\n",
      "        [-2.1151, -2.0421, -2.0883, -2.1581, -2.1212, -2.0197, -2.0214, -2.0783],\n",
      "        [-2.0683, -2.0992, -2.0486, -2.0950, -2.1395, -2.0397, -2.0194, -2.1323],\n",
      "        [-2.0556, -2.0997, -2.1066, -2.0898, -2.0724, -2.0421, -2.0250, -2.1499]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "train_groups_indices, train_groups_maxes = generate_batches(train_data, BATCH_SIZE)\n",
    "test_groups_indices, test_groups_maxes = generate_batches(test_data, BATCH_SIZE)\n",
    "\n",
    "n_train = len(train_data) // BATCH_SIZE\n",
    "n_test = len(test_data) // BATCH_SIZE\n",
    "\n",
    "batch_x = train_data.iloc[train_groups_indices[1]].tokens.apply(lambda d: \n",
    "                                                                          generate_embeds_with_pads(d, train_groups_maxes[1]) ).values.tolist()\n",
    "batch_y = train_data.loc[train_groups_indices[1]].bin_emotions.values.tolist()\n",
    "\n",
    "final_batch_x = torch.FloatTensor(np.array(batch_x))\n",
    "final_batch_y = torch.FloatTensor(np.array(batch_y))\n",
    "\n",
    "dummy_model = EmoNet(NUM_LAYERS, HIDDEN_SIZE, EMBEDDING_DIM, num_emotions, KEEP_PROB)\n",
    "log_probs = dummy_model(final_batch_x)\n",
    "print(log_probs[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now let's train the model. But first, let's define the necessary variables to conduct the training like the optimizer and whether we are training on the cpu or gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = EmoNet(NUM_LAYERS, HIDDEN_SIZE, EMBEDDING_DIM, num_emotions, KEEP_PROB).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "dimension = EMBEDDING_DIM\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 10, 0, 0, 0\n",
    "\n",
    "### defining batch generation\n",
    "train_groups_indices, train_groups_maxes = generate_batches(train_data, BATCH_SIZE)\n",
    "test_groups_indices, test_groups_maxes = generate_batches(test_data, BATCH_SIZE)\n",
    "\n",
    "n_train = len(train_data) // BATCH_SIZE\n",
    "n_test = len(test_data) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target, batch_size):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and finally we can train the model. Note that I stopped the training after the first round, since I have already done the training on my computer. You can let the training continue until you have reached a nice accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , pass acc: 0 , current acc: 46\n",
      "time taken: 48.675190687179565\n",
      "epoch: 1 , training loss: 1.4679006251582394 , training acc: 46 , valid loss: 1.490867356731467 , valid acc: 46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-73bd05a24bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         batch_x = train_data.iloc[train_groups_indices[b+1]].tokens.apply(lambda d: \n\u001b[0m\u001b[1;32m     12\u001b[0m                                                                           generate_embeds_with_pads(d, train_groups_maxes[b+1]) ).values.tolist()\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_groups_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_emotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ellfae/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-73bd05a24bcd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         batch_x = train_data.iloc[train_groups_indices[b+1]].tokens.apply(lambda d: \n\u001b[0;32m---> 12\u001b[0;31m                                                                           generate_embeds_with_pads(d, train_groups_maxes[b+1]) ).values.tolist()\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_groups_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_emotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-697b72d5f583>\u001b[0m in \u001b[0;36mgenerate_embeds_with_pads\u001b[0;34m(tokens, max_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpadded_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# do embedding for existing tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mpadded_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### training\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    ### early stoping to avoid overfitting\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc , test_loss = 0, 0, 0, 0\n",
    "    \n",
    "    for b in range(n_train):\n",
    "        batch_x = train_data.iloc[train_groups_indices[b+1]].tokens.apply(lambda d: \n",
    "                                                                          generate_embeds_with_pads(d, train_groups_maxes[b+1]) ).values.tolist()\n",
    "        batch_y = train_data.loc[train_groups_indices[b+1]].bin_emotions.values.tolist()\n",
    "        batch_y = np.argmax(batch_y, axis=1)        \n",
    "        final_batch_x = torch.FloatTensor(np.array(batch_x)).to(device)\n",
    "        final_batch_y = torch.LongTensor(batch_y).to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        y_hat = model(final_batch_x)\n",
    "        \n",
    "        loss = F.nll_loss(y_hat, final_batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += get_accuracy(y_hat, final_batch_y, BATCH_SIZE)\n",
    "        \n",
    "    for b in range(n_test):\n",
    "        batch_x = test_data.iloc[test_groups_indices[b+1]].tokens.apply(lambda d: \n",
    "                                                                          generate_embeds_with_pads(d, test_groups_maxes[b+1]) ).values.tolist()\n",
    "        batch_y = test_data.loc[test_groups_indices[b+1]].bin_emotions.values.tolist()\n",
    "        batch_y = np.argmax(batch_y, axis=1)\n",
    "        final_batch_x = torch.FloatTensor(np.array(batch_x)).to(device)\n",
    "        final_batch_y = torch.LongTensor(batch_y).to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        y_hat = model(final_batch_x)\n",
    "                \n",
    "        loss = F.nll_loss(y_hat, final_batch_y)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_acc += get_accuracy(y_hat, final_batch_y, BATCH_SIZE)\n",
    "        \n",
    "    train_loss /= n_train\n",
    "    train_acc /= n_train\n",
    "    test_loss /= n_test\n",
    "    test_acc /= n_test\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc.cpu().numpy())\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        ### TODO: do checkpoint for model here using PyTorch\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1\n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc.cpu().numpy(), ', valid loss:', test_loss, ', valid acc:', test_acc.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Model\n",
    "Now that the model has been trained, we can store the classifier and then reuse it again to classify sentences or other tweets in the future. We will do this in the next chapter of this series. By the way, notice that I didn't properly evaluate the performance of the model here. I am sure you can find a way to improve the accuracy of the model by using more advanced deep learning techniques. You can also try to find a method to properly evaluate the model. I will provide that code in a future chapter. For now, we will use the model above, which has a fair accuracy, since the purpose of the series is to show you how to use the inferences of the model to conduct further analysis on a new dataset. We will cover this further analysis in the next chapter. Let's store the model first, and then we will retrieve it in the next notebook for classifying real-time tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellfae/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type EmoNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tmodel = copy.deepcopy(model)\n",
    "torch.save(tmodel, 'model/elastic_hashtag_model/emonet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
