{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticsearch Restoring Emotion Classifier/Crawling\n",
    "Here we are going to resotre the emotion classifier that we trained in the previous chapter of the series. Then we will crawl real-time tweets from Twitter to classify, and then index into Elasticsearch. Then we are going to use Kibana to analyze the predictions and see where the model is doing well and not so well. In fact, we are going to use the inference of the model, to answer a few interesting questions using Kibana powerful analytic functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import helpers.pickle_helpers as ph\n",
    "import time\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Let's define the hyperparameters again since we need then when restoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000 # max size of vocabulary\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "ATTENTION_SIZE = 150\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50 # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Embeddings\n",
    "Also, we will need the word embeddings to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load word embeddings and accompanying vocabulary\n",
    "wv = ph.load_from_pickle(\"data/hashtags_word_embeddings/es_py_cbow_embeddings.p\")\n",
    "vocab = ph.load_from_pickle(\"data/hashtags_word_embeddings/es_py_cbow_dictionary.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine Model\n",
    "Since we used a very a naive way to store our model in the previous chapter, we will need to redefine the same model again. With the latest version of PyTorch there better ways to store and restore models without having to do all of this unecessary steps. For now, let's just use this simple approach to restore our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoNet(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, embedding_dim, output_size, dropout):\n",
    "        super(EmoNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.keep_prob = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = num_layers\n",
    "        self.output = output_size\n",
    "        \n",
    "        self.dropout  = nn.Dropout(p=self.keep_prob)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=self.embedding_dim,\n",
    "                                 hidden_size=self.hidden_size, \n",
    "                                 num_layers=self.nlayers,\n",
    "                                 dropout=self.keep_prob)\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # batch_size X seq_len X embedding_dim -> seq_len, batch_size, embedding_dim\n",
    "        X = inputs.permute(1,0,2)\n",
    "        self.rnn.flatten_parameters()\n",
    "        output, hidden = self.rnn(X)\n",
    "        (_, last_state) = hidden      \n",
    "        out = self.dropout(output[-1])  \n",
    "        out = self.linear(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### restoring the model\n",
    "tmodel = torch.load('model/elastic_hashtag_model/emonet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "For simplicity, let's redefine the helper function we used before. If you want to further optimize your code, you could easily put these reusable functions into a seperate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: move this preprocessing helper functions\n",
    "def clearstring(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub('[^\\'\\\"A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = [y for y in string if len(y) > 3 and y.find('nbsp') < 0]\n",
    "    return string\n",
    "\n",
    "def generate_embeds_with_pads(tokens, max_size):\n",
    "   \n",
    "    padded_embedding = []\n",
    "    for i in range(max_size):\n",
    "        if i+1 > len(tokens): # do padding\n",
    "            padded_embedding.append(list(np.zeros(EMBEDDING_DIM)))\n",
    "        else: # do embedding for existing tokens\n",
    "            padded_embedding.append(list(wv[vocab[tokens[i]]]))  \n",
    "    return padded_embedding\n",
    "\n",
    "def remove_unknown_words(tokens):\n",
    "    return [t for t in tokens if t in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Transformations\n",
    "When we are crawling data from the Twitter API, we need to preprocess it and then transform the input to word embedding representations. Same process as used for the classifier int the previous chapter, just that this case we are using it to classify real-time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_to_input(text):\n",
    "    \"\"\" Accepts only one text as input; can be done by batches later on\"\"\"\n",
    "    ### TODO Do the preprocessing here\n",
    "    text = clearstring(text) # list of tokens\n",
    "    text = remove_unknown_words(text)\n",
    "    emb = generate_embeds_with_pads(text, len(text))\n",
    "    \n",
    "    return emb\n",
    "\n",
    "emo_map = {0: 'anger', \n",
    "           1: 'anticipation', \n",
    "           2: 'disgust', \n",
    "           3: 'fear', \n",
    "           4: 'joy', \n",
    "           5: 'sadness',\n",
    "           6: 'surprise',\n",
    "           7: 'trust'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Classification of text\n",
    "Let's test to see if those function above work on a dummy text. Wow! You can see that the classifier classifies the word \"unhappy\" to sadness, which means the model is good to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the emotion from tweet\n",
    "x = transform_data_to_input(\"unhappy\") # put tweet here\n",
    "final_x = torch.FloatTensor(np.array(x))\n",
    "final_x = final_x.unsqueeze(0)\n",
    "emo_map[torch.argmax(tmodel(final_x.to(device))).detach().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Data And Index to Elasticsearch\n",
    "Now to the main part of this series. We have pretrained embeddings, we have trained and stored a classifier, but the best part of all will happen next. We will crawl real-time tweets and classify them into an emotion. We will then store those inferences of the model, along with the text, into Elasticsearch. We will then connect the Elasticsearch with Kibana and analyze our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import a few useful libraries\n",
    "import crawlers.config as cf\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "es = Elasticsearch(cf.ELASTICSEARCH['hostname'])\n",
    "import sys, json\n",
    "import crawlers.config as config\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Configurations\n",
    "The following are just some extra configurations that are needed for the crawler. Keep in mind that these configurations are mostly obtained from the config library provided with the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en']\n",
    "WANTED_KEYS = [\n",
    "    'id_str',\n",
    "    'text',\n",
    "    'created_at',\n",
    "    'in_reply_to_status_id_str',\n",
    "    'in_reply_to_user_id_str',\n",
    "    'retweeted',\n",
    "    'entities']  # Wanted keys to store in the database\n",
    "KEYWORDS = config.KEYWORDS['joy'] + \\\n",
    "           config.KEYWORDS['trust'] + \\\n",
    "           config.KEYWORDS['fear'] + \\\n",
    "           config.KEYWORDS['surprise'] + \\\n",
    "           config.KEYWORDS['sadness'] + \\\n",
    "           config.KEYWORDS['disgust'] + \\\n",
    "           config.KEYWORDS['anger'] + \\\n",
    "           config.KEYWORDS['anticipation'] + \\\n",
    "           config.KEYWORDS['other']\n",
    "\n",
    "print(len(KEYWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Below are a few helper functions which will be useful for the crawler in order to properly store the information we want on Elasticsearch. Again, the notebook could be simplified by putting this code in a seperate Python file. For now, we will stick to our long functions just to have everything in one place, where I can easily explain the components of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_es_format(tweet):\n",
    "    \"\"\"Convert into elastic format\"\"\"\n",
    "    action = [\n",
    "        {\n",
    "            \"_index\": cf.ELASTICSEARCH['index'],\n",
    "            \"_type\": cf.ELASTICSEARCH['type'],\n",
    "            \"_source\": {\n",
    "                \"emotion\": tweet[\"emotion\"],\n",
    "                \"created_at\": tweet[\"created_at\"],\n",
    "                \"tweet_id\": tweet[\"tweet_id\"],\n",
    "                \"text\": tweet[\"text\"],\n",
    "                \"hashtags\": tweet[\"hashtags\"]                 \n",
    "            }          \n",
    "        }\n",
    "    ]\n",
    "    return action\n",
    "\n",
    "def post_tweet_to_es(doc):\n",
    "    \"\"\" insert into Elasticsearch in bulk \"\"\"\n",
    "    helpers.bulk(es, doc)\n",
    "\n",
    "def get_hashtags(list):\n",
    "    \"\"\"obtain hashtags from tweet\"\"\"\n",
    "    hashtags = []\n",
    "    for h in list:\n",
    "        hashtags.append(h['text'])\n",
    "    return hashtags\n",
    "\n",
    "def predict_emotion(text):\n",
    "    \"\"\" ouput prediction of the model \"\"\"\n",
    "    x = transform_data_to_input(text) # put tweet here\n",
    "    final_x = torch.FloatTensor(np.array(x))\n",
    "    final_x = final_x.unsqueeze(0)\n",
    "    return emo_map[torch.argmax(tmodel(final_x.to(device))).detach().item()]\n",
    "\n",
    "def format_to_print(tweet, hashtags):\n",
    "    \"\"\" format raw tweet \"\"\"\n",
    "    tweet_dict = {'text':tweet['text'],\n",
    "            'created_at': tweet['created_at'],\n",
    "            'tweet_id': tweet['id_str'],\n",
    "            'emotion': predict_emotion(tweet['text']),\n",
    "            'hashtags': hashtags}\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Crawler\n",
    "And we are finally ready to start crawling and storing our data. The crawler code below is standard code for crawling from the Twitter API. You will need to configure all your tokens in the config file so that this code can work. The `on_data` function in the class below achives everything we want: from preprocessing it, to classifying it, to indexing it into Elasticsearch. Spend some time analyzing the code below and make sure you understand how it is doing everything which I just explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Listener\n",
    "class Listener(StreamListener):\n",
    "\n",
    "    @staticmethod\n",
    "    def on_data(data):\n",
    "        try:\n",
    "            reponse = json.loads(data)\n",
    "            tweet = {key: reponse[key] for key in set(WANTED_KEYS) & set(reponse.keys())}\n",
    "\n",
    "            hashtags = get_hashtags(tweet['entities']['hashtags'])\n",
    "            anyretweet= re.findall(r'RT|https|http', str(tweet['text']))\n",
    "\n",
    "            ### formatting tweet\n",
    "            final_tweet = format_to_print(tweet, hashtags)\n",
    "\n",
    "            ### make insertions\n",
    "            if not anyretweet:                                \n",
    "                f = deepcopy(final_tweet)\n",
    "                \n",
    "                ### insert to elasticsearch\n",
    "                es_final_tweet = convert_to_es_format(f)\n",
    "                post_tweet_to_es(es_final_tweet)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print (\"--------------On data function------------\")\n",
    "            return True\n",
    "\n",
    "    @staticmethod\n",
    "    def on_error(status):\n",
    "        print (\"--------------On error function------------\")\n",
    "        print (status)\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def on_timeout():\n",
    "        print (\"--------------On timeout function------------\")\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True  # Don't kill the stream\n",
    "\n",
    "    @staticmethod\n",
    "    def on_status(status):\n",
    "        print (\"--------------On status function------------\")\n",
    "        print (status.text)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Crawling...\n",
    "And for the moment you have been waiting for. Let's start the crawler! You can let the crawler run for as much time as you want. Since this is a crawler, I do suggest you convert this notebook into a Python script to make it more efficient. You will also notice sometimes that the crawler will output a warning \"cannot unsqueeze empty tensor\", you can ignore it since sometimes tweets are too short to deduce any information from them and so the classifier won't be able to infer anything from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starts streaming\n",
    "while True:\n",
    "    try:\n",
    "        auth = OAuthHandler(\n",
    "            config.TWITTER['consumer_key'], config.TWITTER['consumer_secret'])\n",
    "        auth.set_access_token(\n",
    "            config.TWITTER['access_token'], config.TWITTER['access_secret'])\n",
    "        print(\"Crawling, Classifying, and Indexing tweets...\")\n",
    "        twitterStream = Stream(auth, Listener())\n",
    "        twitterStream.filter(languages=LANGUAGES, track=KEYWORDS)\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"--------------On keyboard interruption function------------\")\n",
    "        print(\"Bye\")\n",
    "        sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
