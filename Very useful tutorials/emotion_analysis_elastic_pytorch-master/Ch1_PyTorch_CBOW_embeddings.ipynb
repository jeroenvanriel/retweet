{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for CBOW Word Embeddings\n",
    "In this notebook we would like to employ the CBOW algorithm to output pretrained word embeddings, which will be used to perform text classification. The text classification will be performed on real-time tweets obtained from the Twitter Public Streaming API. Once we have classified the tweet, we can then further process them and index (store) in an Elasticsearch index (data storage). The Elasticsearch index will serve as a real-time, fast search engine, which will allows us to perform different kinds of analyses and visualizations on the classified and preprocessed tweets using Kibana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "The first thing is to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import helpers.pickle_helpers as ph\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### checking PyTorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "The data has been preprocessed, cleaned, and formated as a pandas dataframe. Below you can see the code used for importing the dataframes. Dataframes allow us to perform basic statistics and transformations to our dataset. Although it is not mandatory to have our data in such format, I strongly recommend it for in case you like to perform some extra analysis on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the traing and test dataframes\n",
    "train_data = ph.load_from_pickle(directory=\"data/datasets/df_grained_tweet_tr.pkl\")\n",
    "test_data = ph.load_from_pickle(directory=\"data/datasets/df_grained_tweet_te_unbal.pkl\")\n",
    "\n",
    "###  renaming the column names of the dataframe\n",
    "train_data.rename(index=str, columns={\"emo\":\"emotions\", \"sentence\": \"text\"}, inplace=True);\n",
    "test_data.rename(index=str, columns={\"emo\":\"emotions\", \"sentence\": \"text\"}, inplace=True);\n",
    "\n",
    "### remove hashtags to avoid any bias from them\n",
    "train_data.text = train_data.text.str.replace(\" <hashtag>\", \"\")\n",
    "test_data.text = test_data.text.str.replace(\" <hashtag>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Distribution\n",
    "We can now explore the distribution of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cba469320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.emotions.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Creating Vocabulary\n",
    "Before training and constructing the word embeddings we need to process the data and create a vocabulary. The functions below achieve these two things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    \"\"\" clean tweets \"\"\"\n",
    "    string = re.sub('[^\\'\\\"A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = [y for y in string if len(y) > 3 and y.find('nbsp') < 0]\n",
    "    return ' '.join(string)\n",
    "\n",
    "def read_data():\n",
    "    \"\"\" generate vocabulary \"\"\"\n",
    "    vocab = []\n",
    "    text = train_data.text.values.tolist()\n",
    "    for t in text:\n",
    "        strings = clearstring(t)\n",
    "        vocab+=strings.split()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 10 words: ['love', 'with', 'midi', 'skirt', 'woooo', 'usermention', 'holme', 'upon', 'spalding', 'moor']\n",
      "size corpus: 3698224\n",
      "size of unique words: 92095\n"
     ]
    }
   ],
   "source": [
    "### build the vocabulary\n",
    "vocabulary = read_data()\n",
    "print(\"example 10 words:\", vocabulary[:10])\n",
    "print('size corpus:',len(vocabulary))\n",
    "vocabulary_size = len(list(set(vocabulary)))\n",
    "print('size of unique words:',vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'with',\n",
       " 'midi',\n",
       " 'skirt',\n",
       " 'woooo',\n",
       " 'usermention',\n",
       " 'holme',\n",
       " 'upon',\n",
       " 'spalding',\n",
       " 'moor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print the vocabulary\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Dataset\n",
    "Now that we have the vocabulary, we would like to format our dataset into sequence of tokens represented by the index of those tokens in the vocabulary. Look at the code below to see how this can be achieved. In particular note the example of how to convert the data to sequence of indexed tokens. The reason the index is necessary is because this is needed by the model in order to identify words by their ids instead of their actual raw representation (i.e., letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # increase index as words added\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1], ('usermention', 147536), ('this', 52001), ('just', 51505), ('that', 50171)]\n",
      "Sample data [34, 5, 16555, 3984, 2340, 1, 30165, 2511, 30166, 24865, 1075, 1815, 161, 97, 117] ['love', 'with', 'midi', 'skirt', 'woooo', 'usermention', 'holme', 'upon', 'spalding', 'moor', 'rock', 'challenge', 'please', 'well', 'done']\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:15], [reverse_dictionary[i] for i in data[:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3698224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches\n",
    "For training the CBOW model with the input sequences of tokens, we need to generate batches and feed those to the model. The batches will be of a particular size, in this case 8, and also context window, which determines the final size of the input sequence. For instance, if the context window size is 1, then we have a target and two context words, which means that the sequence size results in three tokens. See the code below for how to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, context_window):\n",
    "    # all context tokens should be used, hence no associated num_skips argument\n",
    "    global data_index\n",
    "    context_size = 2 * context_window\n",
    "    batch = np.ndarray(shape=(batch_size, context_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * context_window + 1  # [ context_window target context_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size):\n",
    "        # context tokens are just all the tokens in buffer except the target\n",
    "        batch[i, :] = [token for idx, token in enumerate(buffer) if idx != context_window]\n",
    "        labels[i, 0] = buffer[context_window]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    data_index-=1\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, context_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 34),\n",
       " ('with', 5),\n",
       " ('midi', 16555),\n",
       " ('skirt', 3984),\n",
       " ('woooo', 2340),\n",
       " ('usermention', 1),\n",
       " ('holme', 30165),\n",
       " ('upon', 2511),\n",
       " ('spalding', 30166),\n",
       " ('moor', 24865)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(reverse_dictionary[i],i) for i in data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 5, 16555, 3984, 2340, 1, 30165, 2511, 30166, 24865]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 love 16555 midi -> 5 with\n",
      "5 with 3984 skirt -> 16555 midi\n",
      "16555 midi 2340 woooo -> 3984 skirt\n",
      "3984 skirt 1 usermention -> 2340 woooo\n",
      "2340 woooo 30165 holme -> 1 usermention\n",
      "1 usermention 2511 upon -> 30165 holme\n",
      "30165 holme 30166 spalding -> 2511 upon\n",
      "2511 upon 24865 moor -> 30166 spalding\n"
     ]
    }
   ],
   "source": [
    "### a batch sample\n",
    "for i in range(8):\n",
    "    print(batch[i, 0], reverse_dictionary[batch[i, 0]],\n",
    "          batch[i, 1], reverse_dictionary[batch[i, 1]],\n",
    "          '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Now that we have a function to generate batches of sequences, which we can feed to the mode, it's time to actually build the model. The model we will use here is a one layer feed forward neural network, which train with the goal to learn parameters that will represent the values of the word embeddings. See code below for the model we will use to train the embeddings. It is assumed that you are familiar with PyTorch to understand the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(EmoCBOW, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # batch_size X context X embedding_dim\n",
    "        embeds = self.embeddings(inputs)\n",
    "        average_embeds = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(average_embeds) \n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def get_word_emdedding(self, word):\n",
    "        word = Variable(torch.LongTensor([dictionary[word]]).cuda())\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretesting Model\n",
    "We would like to test the model with a sample batch before we do the actual training of the word embeddings. Below is sample code on how to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 92095])\n"
     ]
    }
   ],
   "source": [
    "### test with one batch\n",
    "context_batch, target_batch = generate_batch(batch_size=8, context_window=1)\n",
    "\n",
    "context_var = Variable(torch.LongTensor(context_batch))\n",
    "### print(context_var)\n",
    "dummy_model = EmoCBOW(vocab_size=vocabulary_size, embedding_dim=10, context_size=1)\n",
    "log_probs = dummy_model(context_var)\n",
    "print(log_probs.size())\n",
    "\n",
    "data_index = 0 # reset index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now it's time to train the model. Again, I assume you are familiar with the particular bit you need to perform the training with PyTorch. While the model is training you see an output loss which indicates whether the model is learning. We train for a lot of steps, so be patient and if everything went right, you should see the loss decreasing and the model converging, which means that the word embeddings are getting better. I have stopped in the process below, but you can let it train for a couple of steps, or even let it run for the full number of steps I defined. The time of training will vary but usually it takes a while for the model to train the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0 :  11.477616\n",
      "Average loss at step 2000 :  9.01468\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6aa07c3fbcb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONTEXT_WINDOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcontext_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 128\n",
    "CONTEXT_WINDOW = 2\n",
    "num_steps = 100000\n",
    "plot_every = 2000\n",
    "\n",
    "losses = []\n",
    "model = EmoCBOW(vocabulary_size, EMBEDDING_DIM, CONTEXT_WINDOW)\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "average_loss = 0\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(BATCH_SIZE, CONTEXT_WINDOW)\n",
    "    context_var = Variable(torch.LongTensor(batch_inputs).cuda())\n",
    "    targets = Variable(torch.squeeze(torch.LongTensor(batch_labels).cuda()))\n",
    "    model.zero_grad()\n",
    "   \n",
    "    log_probs = model(context_var) # inputs are context vectors\n",
    "    loss = F.nll_loss(log_probs, targets) # criterion\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    average_loss += loss.data\n",
    "    \n",
    "    if step % plot_every == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= plot_every\n",
    "        print(\"Average loss at step\", step, \": \", average_loss.cpu().numpy())\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Embeddings\n",
    "After you have trained the embeddings, you know want to store them and reuse them for some downstream task like sentiment classification. Essentially, you will be using the embeddings as input features for a model to conduct some task. The format you store the embeddings doesn't really matter, for as long as you can retrieve them easily and efficiently in the future. I am storing the embeddings in numpy in this notebook and further converting the matrix into pickle file. Notice that we are also storing the vocabulary since we will need it again when training our text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.convert_to_pickle(directory=\"data/hashtags_word_embeddings/es_py_cbow_embeddings.p\", item=embeddings)\n",
    "ph.convert_to_pickle(directory=\"data/hashtags_word_embeddings/es_py_cbow_dictionary.p\", item=dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
