{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iIGjBVxfs_m"
   },
   "source": [
    "## Part 4 - Applying Modern Deep Learning to NLP\n",
    "\n",
    "**Total time: 60 minutes**\n",
    "\n",
    "This notebook corresponds to the main block of the tutorial where we will cover how to build and train deep learning architectures, such as RNN, for an NLP task. The task at hand is emotion classification which is a multi-class problem. Let's dive in!\n",
    "\n",
    "---\n",
    "\n",
    "### Journey\n",
    "\n",
    "- Load the Data\n",
    "- Implementing Model\n",
    "- Pretesting Model\n",
    "- Setup Training\n",
    "- Traing Model\n",
    "- Storing Model\n",
    "- **Exercise:** Implementing Your Own Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6mdHUdLdHTf"
   },
   "source": [
    "### Load the Data\n",
    "Instead of reloading the data, we restore it from the previous phase.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BBLZ0sFdE9G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# helper functions\n",
    "def convert_to_pickle(item, directory):\n",
    "    pickle.dump(item, open(directory,\"wb\"))\n",
    "\n",
    "\n",
    "def load_from_pickle(directory):\n",
    "    return pickle.load(open(directory,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pKt8eof-IqgA",
    "outputId": "20a23a56-7f6f-47a0-afb9-7e5f99848820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "# data instance\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x, y, x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_folder = \"/gdrive/My Drive/pycon2019/\"\n",
    "\n",
    "train_dataset = load_from_pickle(data_folder + \"train_dataset\")\n",
    "test_dataset = load_from_pickle(data_folder + \"test_dataset\")\n",
    "val_dataset = load_from_pickle(data_folder + \"val_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xtHq06XGJkV4",
    "outputId": "8a9496c8-8d77-4684-903a-da26d6186822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP3VrH2h5Q7C"
   },
   "source": [
    "### Implementing Model\n",
    "\n",
    "After the data has been preprocessed, transformed and prepared it is now time to construct the model or the so-called computation graph that will be used to train our classification models. We are going to use a gated recurrent neural network (GRU), which is considered a more efficient version of a basic RNN. The figure below shows a high-level overview of the model details. \n",
    "\n",
    "![alt txt](https://github.com/omarsar/nlp_pytorch_tensorflow_notebooks/blob/master/img/gru-model.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXwdHHeQfugz"
   },
   "outputs": [],
   "source": [
    "class EmoGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
    "        super(EmoGRU, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
    "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
    "    \n",
    "    def initialize_hidden_state(self, device):\n",
    "        return torch.zeros((1, self.batch_sz, self.hidden_units)).to(device)\n",
    "    \n",
    "    def forward(self, x, lens, device):\n",
    "        x = self.embedding(x)\n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
    "        out = output[-1, :, :] \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out, self.hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzcFnWB07Ev2"
   },
   "source": [
    "### Pretesting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqSmsXuoKDM2"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "TRAIN_BUFFER_SIZE = 40000 # len(input_tensor_train)\n",
    "VAL_BUFFER_SIZE = 5000 # len(input_tensor_val)\n",
    "TEST_BUFFER_SIZE = 5000 # len(input_tensor_test)\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
    "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE\n",
    "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = 27291 # len(inputs.word2idx)\n",
    "target_size = 6 # num_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CekF-k7m7F_i"
   },
   "outputs": [],
   "source": [
    "# sort batch function to be able to use with pad_packed_sequence\n",
    "# batch elements ordered decreasingle by their length\n",
    "\n",
    "def sort_batch(X, y, lengths):\n",
    "    \"sort the batch by length\"\n",
    "    \n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFmJQKBzdn3F"
   },
   "source": [
    "`pad_packed_sequence` is a utility function to efficiently and automatically pad your data of variable length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WziHVOpG7Lcz",
    "outputId": "14d82d51-e0f6-4300-c236-7ed2a514144c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  torch.Size([69, 64])\n",
      "torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
    "model.to(device)\n",
    "\n",
    "# obtain one sample from the data iterator\n",
    "it = iter(train_dataset)\n",
    "x, y, x_len = next(it)\n",
    "\n",
    "# sort the batch first to be able to use with pac_pack sequence\n",
    "xs, ys, lens = sort_batch(x, y, x_len)\n",
    "\n",
    "print(\"Input size: \", xs.size())\n",
    "\n",
    "output, _ = model(xs.to(device), lens, device)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUeoDIDX7Ovk"
   },
   "source": [
    "### Setup Training\n",
    "Now that we have tested the model, it is time to train it. We will define out optimization algorithm, learnin rate, and other necessary information to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeuPVVgw7SZU"
   },
   "outputs": [],
   "source": [
    "# Enabling cuda\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
    "model.to(device)\n",
    "\n",
    "# loss criterion and optimizer for training\n",
    "criterion = nn.CrossEntropyLoss() # the same as log_softmax + NLLLoss\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def loss_function(y, prediction):\n",
    "    \"\"\" CrossEntropyLoss expects outputs and class indices as target \"\"\"\n",
    "    # convert from one-hot encoding to class indices\n",
    "    target = torch.max(y, 1)[1]\n",
    "    loss = criterion(prediction, target) \n",
    "    return loss   #TODO: refer the parameter of these functions as the same\n",
    "    \n",
    "def accuracy(target, logit):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
    "    corrects = (torch.max(logit, 1)[1].data == target).sum()\n",
    "    accuracy = 100.0 * corrects / len(logit)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dc3tkiqQ9JzH"
   },
   "source": [
    "### Training Model\n",
    "\n",
    "Now we finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1767
    },
    "colab_type": "code",
    "id": "ujawKs7x9Lc8",
    "outputId": "e05c58db-2870-4ebe-c0f5-9dc85522b93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Val. Loss 0.2972\n",
      "Epoch 1 Batch 100 Val. Loss 0.2769\n",
      "Epoch 1 Batch 200 Val. Loss 0.2586\n",
      "Epoch 1 Batch 300 Val. Loss 0.1642\n",
      "Epoch 1 Batch 400 Val. Loss 0.0631\n",
      "Epoch 1 Batch 500 Val. Loss 0.0686\n",
      "Epoch 1 Batch 600 Val. Loss 0.0419\n",
      "Epoch 1 Loss 0.1543 -- Train Acc. 63.0000 -- Val Acc. 90.0000\n",
      "Time taken for 1 epoch 25.166279554367065 sec\n",
      "\n",
      "Epoch 2 Batch 0 Val. Loss 0.0258\n",
      "Epoch 2 Batch 100 Val. Loss 0.0387\n",
      "Epoch 2 Batch 200 Val. Loss 0.0269\n",
      "Epoch 2 Batch 300 Val. Loss 0.0115\n",
      "Epoch 2 Batch 400 Val. Loss 0.0357\n",
      "Epoch 2 Batch 500 Val. Loss 0.0150\n",
      "Epoch 2 Batch 600 Val. Loss 0.0317\n",
      "Epoch 2 Loss 0.0281 -- Train Acc. 92.0000 -- Val Acc. 91.0000\n",
      "Time taken for 1 epoch 26.150993585586548 sec\n",
      "\n",
      "Epoch 3 Batch 0 Val. Loss 0.0035\n",
      "Epoch 3 Batch 100 Val. Loss 0.0233\n",
      "Epoch 3 Batch 200 Val. Loss 0.0175\n",
      "Epoch 3 Batch 300 Val. Loss 0.0090\n",
      "Epoch 3 Batch 400 Val. Loss 0.0395\n",
      "Epoch 3 Batch 500 Val. Loss 0.0235\n",
      "Epoch 3 Batch 600 Val. Loss 0.0131\n",
      "Epoch 3 Loss 0.0206 -- Train Acc. 93.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.44951367378235 sec\n",
      "\n",
      "Epoch 4 Batch 0 Val. Loss 0.0066\n",
      "Epoch 4 Batch 100 Val. Loss 0.0127\n",
      "Epoch 4 Batch 200 Val. Loss 0.0135\n",
      "Epoch 4 Batch 300 Val. Loss 0.0034\n",
      "Epoch 4 Batch 400 Val. Loss 0.0149\n",
      "Epoch 4 Batch 500 Val. Loss 0.0145\n",
      "Epoch 4 Batch 600 Val. Loss 0.0327\n",
      "Epoch 4 Loss 0.0185 -- Train Acc. 94.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.525928258895874 sec\n",
      "\n",
      "Epoch 5 Batch 0 Val. Loss 0.0126\n",
      "Epoch 5 Batch 100 Val. Loss 0.0188\n",
      "Epoch 5 Batch 200 Val. Loss 0.0294\n",
      "Epoch 5 Batch 300 Val. Loss 0.0171\n",
      "Epoch 5 Batch 400 Val. Loss 0.0168\n",
      "Epoch 5 Batch 500 Val. Loss 0.0140\n",
      "Epoch 5 Batch 600 Val. Loss 0.0226\n",
      "Epoch 5 Loss 0.0167 -- Train Acc. 94.0000 -- Val Acc. 91.0000\n",
      "Time taken for 1 epoch 26.54446840286255 sec\n",
      "\n",
      "Epoch 6 Batch 0 Val. Loss 0.0027\n",
      "Epoch 6 Batch 100 Val. Loss 0.0049\n",
      "Epoch 6 Batch 200 Val. Loss 0.0010\n",
      "Epoch 6 Batch 300 Val. Loss 0.0125\n",
      "Epoch 6 Batch 400 Val. Loss 0.0167\n",
      "Epoch 6 Batch 500 Val. Loss 0.0124\n",
      "Epoch 6 Batch 600 Val. Loss 0.0079\n",
      "Epoch 6 Loss 0.0145 -- Train Acc. 95.0000 -- Val Acc. 91.0000\n",
      "Time taken for 1 epoch 26.918611764907837 sec\n",
      "\n",
      "Epoch 7 Batch 0 Val. Loss 0.0107\n",
      "Epoch 7 Batch 100 Val. Loss 0.0119\n",
      "Epoch 7 Batch 200 Val. Loss 0.0125\n",
      "Epoch 7 Batch 300 Val. Loss 0.0089\n",
      "Epoch 7 Batch 400 Val. Loss 0.0172\n",
      "Epoch 7 Batch 500 Val. Loss 0.0411\n",
      "Epoch 7 Batch 600 Val. Loss 0.0056\n",
      "Epoch 7 Loss 0.0145 -- Train Acc. 95.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.822684288024902 sec\n",
      "\n",
      "Epoch 8 Batch 0 Val. Loss 0.0121\n",
      "Epoch 8 Batch 100 Val. Loss 0.0082\n",
      "Epoch 8 Batch 200 Val. Loss 0.0023\n",
      "Epoch 8 Batch 300 Val. Loss 0.0054\n",
      "Epoch 8 Batch 400 Val. Loss 0.0132\n",
      "Epoch 8 Batch 500 Val. Loss 0.0046\n",
      "Epoch 8 Batch 600 Val. Loss 0.0066\n",
      "Epoch 8 Loss 0.0117 -- Train Acc. 96.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.59833526611328 sec\n",
      "\n",
      "Epoch 9 Batch 0 Val. Loss 0.0083\n",
      "Epoch 9 Batch 100 Val. Loss 0.0038\n",
      "Epoch 9 Batch 200 Val. Loss 0.0109\n",
      "Epoch 9 Batch 300 Val. Loss 0.0281\n",
      "Epoch 9 Batch 400 Val. Loss 0.0013\n",
      "Epoch 9 Batch 500 Val. Loss 0.0092\n",
      "Epoch 9 Batch 600 Val. Loss 0.0216\n",
      "Epoch 9 Loss 0.0103 -- Train Acc. 97.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.5997257232666 sec\n",
      "\n",
      "Epoch 10 Batch 0 Val. Loss 0.0041\n",
      "Epoch 10 Batch 100 Val. Loss 0.0033\n",
      "Epoch 10 Batch 200 Val. Loss 0.0409\n",
      "Epoch 10 Batch 300 Val. Loss 0.0189\n",
      "Epoch 10 Batch 400 Val. Loss 0.0023\n",
      "Epoch 10 Batch 500 Val. Loss 0.0034\n",
      "Epoch 10 Batch 600 Val. Loss 0.0001\n",
      "Epoch 10 Loss 0.0088 -- Train Acc. 97.0000 -- Val Acc. 92.0000\n",
      "Time taken for 1 epoch 26.909265756607056 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    ### Initialize hidden state\n",
    "    # TODO: do initialization here.\n",
    "    total_loss = 0\n",
    "    train_accuracy, val_accuracy = 0, 0\n",
    "    \n",
    "    ### Training\n",
    "    for (batch, (inp, targ, lens)) in enumerate(train_dataset):\n",
    "        loss = 0\n",
    "        predictions, _ = model(inp.permute(1 ,0).to(device), lens, device) # TODO:don't need _   \n",
    "              \n",
    "        loss += loss_function(targ.to(device), predictions)\n",
    "        batch_loss = (loss / int(targ.shape[1]))        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
    "        train_accuracy += batch_accuracy\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Val. Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.cpu().detach().numpy()))\n",
    "            \n",
    "    ### Validating\n",
    "    for (batch, (inp, targ, lens)) in enumerate(val_dataset):        \n",
    "        predictions,_ = model(inp.permute(1, 0).to(device), lens, device)        \n",
    "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
    "        val_accuracy += batch_accuracy\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
    "                                                             total_loss / TRAIN_N_BATCH, \n",
    "                                                             train_accuracy / TRAIN_N_BATCH,\n",
    "                                                             val_accuracy / VAL_N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnAM43gri7LJ"
   },
   "source": [
    "### Stopping the Model\n",
    "\n",
    "How do we know when to stop the model. We can use a technique called `early stopping`, not covered here, but widely used in deep learning, to control the convergence of models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Part 4 - Applying modern deep learning to NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
